{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b61492-7fc2-44ac-8a46-e40475838337",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd549df-a455-4a08-85cd-d9946bf9655f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Load the fitted CountVectorizer\n",
    "with open('src/fitted_vectorizer_combined.pkl', 'rb') as vectorizer_file:\n",
    "    fitted_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "# Load the Logistic Regression model\n",
    "with open('src/logistic_model.pkl', 'rb') as model_file:\n",
    "    logistic_model = pickle.load(model_file)\n",
    "\n",
    "# Load the Logistic Regression modelwith Hyperparameters\n",
    "with open('src/fitted_gridsearch.pkl', 'rb') as fitted_model_file:\n",
    "    fitted_logistic_model = pickle.load(fitted_model_file)\n",
    "    \n",
    "# Load the fitted TF-IDFCountVectorizer\n",
    "with open('src/fitted_tfidf_vectorizer_combined.pkl', 'rb') as tfidf_vectorizer_file:\n",
    "    tfidf_fitted_vectorizer = pickle.load(tfidf_vectorizer_file)\n",
    "    \n",
    "# Load the TF-IDF Logistic Regression model\n",
    "with open('src/logistic_model_tfidf.pkl', 'rb') as tfidf_model_file:\n",
    "    tfidf_logistic_model = pickle.load(tfidf_model_file)  \n",
    "\n",
    "# Load the Random Forest model\n",
    "with open('src/random_forest_fitted_gridsearch.pkl', 'rb') as random_forest_model_file:\n",
    "    rf_model = pickle.load(random_forest_model_file)  \n",
    "    \n",
    "# Load the SVM Regression model\n",
    "with open('src/svm_fitted_gridsearch.pkl', 'rb') as svm_model_file:\n",
    "    svm_model = pickle.load(svm_model_file)\n",
    "    \n",
    "# Load custom stopwords\n",
    "with open('src/custom_stopwords.pkl', 'rb') as stopwords_file:\n",
    "    stopwords = pickle.load(stopwords_file)\n",
    "\n",
    "# vect,lr = joblib.load(\"combined.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013814b3-9942-4707-a2e6-9421a05ef1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041d4a92-6dc5-49ba-a7cc-7672968a8ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloads\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Other nltk modules\n",
    "from nltk.corpus import stopwords\n",
    "import nltk as nlp\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define  customized stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend([\"rt\", \"u\", \"r\", \"amp\", \"w\", \"th\"])  # Add additional stopwords\n",
    "\n",
    "# remove some stopwords\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('is')\n",
    "stopwords.remove('against')\n",
    "stopwords.remove(\"don't\")\n",
    "stopwords.remove(\"have\")\n",
    "stopwords.remove(\"won't\")\n",
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d593697c-f74e-4d63-bd2b-7a0c654b39b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    # Create a lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Replace \"n't\" with \"not\"\n",
    "    tweet = tweet.replace(\"n't\", \" not\")\n",
    "\n",
    "    # Remove @ sign and all handles\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\", \"\", tweet)\n",
    "\n",
    "    # Remove hashtags and words that come together with hashtags\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "\n",
    "    # Remove all punctuation. It also removes hashtags.\n",
    "    tweet = re.sub(\"[!@$%^&*()_+\\|/?,.:;'`â€™-]+\", \" \", tweet)\n",
    "\n",
    "    # Remove links\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "\n",
    "    # Lowercase all characters\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    tweet = \" \".join(tweet.split())\n",
    "\n",
    "    # Remove stopwords\n",
    "    tweet = ' '.join([word for word in tweet.split() if word.lower() not in stopwords])\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tweet = \" \".join(lemmatized_tokens)\n",
    "\n",
    "    # Remove three dots from the final word, preserving the word itself\n",
    "    tweet = re.sub(r'(\\S+)\\.{2,}$', r'\\1', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f633cb-d6ef-4562-88a6-43e4561dcdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\erick\\aanaconda3\\lib\\site-packages (0.17.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\erick\\appdata\\roaming\\python\\python311\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\erick\\aanaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\erick\\aanaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\erick\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erick\\aanaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\erick\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Text Blob\n",
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "# Sentiment classification function\n",
    "def classify_sentiment(polarity):\n",
    "    if polarity > 0.15:\n",
    "        return 'Positive'\n",
    "    elif polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f93e732-42df-43ba-8e7f-2067717acd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "46eb2513-7a52-46fc-8a8c-ea2021d4072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tweet analysis function\n",
    "\n",
    "def tweet_analysis(tweet, vectorizer=fitted_vectorizer, model=logistic_model):\n",
    "    # Clean the tweet\n",
    "    cleaned_tweet = clean_tweets(tweet)\n",
    "    \n",
    "    # Vectorize the tweet\n",
    "    vectorized_tweet = vectorizer.transform([cleaned_tweet])\n",
    "    \n",
    "    # Transforming the testing data into a DataFrame with feature names as columns\n",
    "    transformed_tweet = pd.DataFrame(vectorized_tweet.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Run Logistic Model\n",
    "    prediction_prob = model.predict_proba(transformed_tweet)[0, 1]  # Probability of being Democrat\n",
    "    \n",
    "   # Determine party affiliation based on threshold\n",
    "    if prediction_prob > 0.511:\n",
    "        party_affiliation = 'Democrat'\n",
    "    elif prediction_prob < 0.491:\n",
    "        party_affiliation = 'Republican'\n",
    "    else:\n",
    "        party_affiliation = 'Neutral'\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiment_polarity = TextBlob(cleaned_tweet).sentiment.polarity\n",
    "    sentiment_label = classify_sentiment(sentiment_polarity)\n",
    "    \n",
    "    results = {\n",
    "        'tweet': tweet,\n",
    "        'cleaned_tweet': cleaned_tweet,\n",
    "        'party_affiliation': party_affiliation,\n",
    "        'tweet_sentiment': sentiment_label\n",
    "    }\n",
    "    \n",
    "    return f\"tweet: {results['tweet']}\\nCleaned Tweet: {results['cleaned_tweet']}\\nParty Affiliation: {results['party_affiliation']}\\nTweet Sentiment: {results['tweet_sentiment']}\\nPredict proba: {prediction_prob}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8cf1eed1-86e3-4875-b8d1-ed28cc03f71e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet: I support border protection\n",
      "Cleaned Tweet: support border protection\n",
      "Party Affiliation: Neutral\n",
      "Tweet Sentiment: Neutral\n",
      "Predict proba: 0.49756072097819337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erick\\aanaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test results\n",
    "\n",
    "# Choose between some of the following vectorizers and model. Otherwise will default to logistic_model and fitted_vectorizer\n",
    "# vectorizer_list = [fitted_vectorizer, tfidf_fitted_vectorizer]\n",
    "# model_list = [logistic_model, fitted_logistic_model, tfidf_logistic_model, rf_model, svm_model]\n",
    "\n",
    "\n",
    "results = tweet_analysis(\"I support border protection\", vectorizer=fitted_vectorizer, model=logistic_model)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710e065-9926-4fb5-8ef8-da0114c6f3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
